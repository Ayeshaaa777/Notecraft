{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "import spacy\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "     \n",
    "\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    content_scraped = []  # Move the initialization outside the if block\n",
    "    if response.status_code == 200:\n",
    "        html_content = response.text\n",
    "        # Use BeautifulSoup to parse the HTML\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        # Extract and print additional information\n",
    "        # Customize this part based on the structure of the web page\n",
    "        paragraphs = soup.find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            content_scraped.append(paragraph.text)\n",
    "    return content_scraped\n",
    "\n",
    "def google_custom_search(query, api_key, cx):\n",
    "    base_url = \"https://www.googleapis.com/customsearch/v1\"\n",
    "    params = {\n",
    "        'key': api_key,\n",
    "        'cx': cx,\n",
    "        'q': query,\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    results = response.json()\n",
    "    content_final=[]\n",
    "\n",
    "    if 'items' in results:\n",
    "        for item in results['items']:\n",
    "            title = item.get('title', '')\n",
    "            link = item.get('link', '')\n",
    "            print(f\"Title: {title}\")\n",
    "            print(f\"Link: {link}\")\n",
    "\n",
    "\n",
    "            # Web scraping\n",
    "            if link:\n",
    "                web_content = scrape_page(link)\n",
    "                content_final.append(web_content)\n",
    "    return content_final\n",
    "     \n",
    "\n",
    "def getQuery():\n",
    "  x=input(\"What do you want to search for?\")\n",
    "  return x\n",
    "     \n",
    "api_key = 'AIzaSyCLi9vqUxLH6ZtKoj5R6dXVAVrgHYMusaI'\n",
    "cx = 'c7f0af2b630ba4893'\n",
    "query = getQuery()\n",
    "content= google_custom_search(query, api_key, cx)\n",
    "\n",
    "def preprocess_texts(list_of_texts):\n",
    "    # Convert list of lists to a single string\n",
    "    raw_text = ' '.join([' '.join(text) for text in list_of_texts])\n",
    "\n",
    "    # Remove HTML tags\n",
    "    cleaned_text = re.sub(r'<.*?>', '', raw_text)\n",
    "\n",
    "    # Remove special characters (retain alphanumeric and spaces)\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s\\.\\?!]', '', cleaned_text)\n",
    "\n",
    "    # Tokenization (splitting into sentences using various punctuation marks)\n",
    "    sentences = re.split(r'[.!?]', cleaned_text)\n",
    "\n",
    "    # Remove empty strings from the list\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    # Join sentences into a single string\n",
    "    processed_text = '.'.join(sentences)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "document = preprocess_texts(content)\n",
    "print(document)\n",
    "     \n",
    "def named_entity_recognition(text):\n",
    "    # Load the spaCy NER model (you can use a larger model for better accuracy)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # Process the text with the NER model\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract named entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "    return entities\n",
    "\n",
    "     \n",
    "\n",
    "entities= named_entity_recognition(document)\n",
    "     \n",
    "\n",
    "print(entities)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# def extract_keywords(text):\n",
    "#     # Convert ENGLISH_STOP_WORDS to a list\n",
    "#     stop_words_list = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "#     # Use TF-IDF for keyword extraction\n",
    "#     tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_list, ngram_range=(1, 1))\n",
    "#     tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "#     # Get feature names (terms)\n",
    "#     feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "#     # Get the top N terms with the highest TF-IDF scores\n",
    "#     top_keywords = [feature_names[idx] for idx in tfidf_matrix.sum(axis=0).argsort()[0, ::-1][:10]]\n",
    "\n",
    "#     return top_keywords\n",
    "import itertools\n",
    "\n",
    "def extract_keywords(text):\n",
    "    # Convert ENGLISH_STOP_WORDS to a list\n",
    "    stop_words_list = list(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    # Use TF-IDF for keyword extraction\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words_list, ngram_range=(1, 1), max_features=10)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "\n",
    "    # Get feature names (terms)\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Get the top N terms with the highest TF-IDF scores\n",
    "    top_keywords = [feature_names[idx] for idx in tfidf_matrix.sum(axis=0).argsort()[0, ::-1][:10]]\n",
    "\n",
    "    return top_keywords[0][0]\n",
    "top_keywords = extract_keywords(document)\n",
    "print(top_keywords)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def extractive_summarization(text, keywords, num_sentences=10):\n",
    "    # Use TF-IDF for sentence representation\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text] + text.split('.'))\n",
    "\n",
    "    # Calculate cosine similarity between sentences\n",
    "    sentence_similarity = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1:])[0]\n",
    "\n",
    "    # Incorporate keywords into sentence importance\n",
    "    for keyword in keywords:\n",
    "        if keyword in tfidf_vectorizer.vocabulary_:\n",
    "            keyword_index = tfidf_vectorizer.vocabulary_[keyword]\n",
    "            # Ensure the shape is compatible for addition\n",
    "            keyword_scores = tfidf_matrix[:, keyword_index].toarray().flatten().reshape(1, -1)\n",
    "\n",
    "            # Check if the shapes match before adding\n",
    "            if keyword_scores.shape == sentence_similarity.shape:\n",
    "                sentence_similarity += keyword_scores\n",
    "\n",
    "    # Get the indices of top N sentences with the highest similarity\n",
    "    top_sentence_indices = sentence_similarity.argsort()[-num_sentences:][::-1]\n",
    "\n",
    "    # Sort the indices to maintain the original order\n",
    "    top_sentence_indices.sort()\n",
    "\n",
    "    # Get the top N sentences\n",
    "    top_sentences = [text.split('.')[i].strip() for i in top_sentence_indices]\n",
    "    return '. '.join(top_sentences)\n",
    "summary= extractive_summarization(document,top_keywords)\n",
    "\n",
    "     print(summary)\n",
    "def post_process_summary(summary):\n",
    "    # Post-processing steps\n",
    "    # 1. Sentence Compression\n",
    "    compressed_summary = '. '.join(set(summary.split('. ')))\n",
    "\n",
    "    # 2. Remove Duplicates\n",
    "    unique_sentences = list(set(compressed_summary.split('. ')))\n",
    "    refined_summary = '. '.join(unique_sentences)\n",
    "\n",
    "    return refined_summary\n",
    "refined_summary = post_process_summary(summary)\n",
    "     \n",
    "\n",
    "print(refined_summary)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
